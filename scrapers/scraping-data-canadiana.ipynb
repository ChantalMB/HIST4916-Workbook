{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rotary-copying",
   "metadata": {},
   "source": [
    "# Scraping Data From Canadiana Collections\n",
    "\n",
    "You can create huge datasets related to Ottawa GLAM institutions by searching the [Canadiana digital collections](https://www.canadiana.ca/). You may have to do additional searching/cleaning after scraping the data to verify it meets your criteria due to the lack of built-in filtering tools in Canadiana's search engine, but that's much easier when all possible relevent information is already in one place/data set!\n",
    "\n",
    "For this example, we're creating a small data set of documents about some of Ottawa's 19th century sporting clubs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install wget\n",
    "\n",
    "# might need this if you use macOS\n",
    "%pip install lxml\n",
    "\n",
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "# search url\n",
    "url = \"https://www.canadiana.ca/search/general/\" + str(page) + \"?q0.0=su%3A%22Ottawa+(Ont.)+--+Clubs.%22\"\n",
    "\n",
    "data = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # The number should be the last page of your results + 1\n",
    "    # on most websites 'if page.status_code != 200:' would work, but for some reason the Canadiana website will let you just... keep going to the next page infinitely even when there's no results\n",
    "    if page > 3:\n",
    "        break\n",
    "    url = \"https://www.canadiana.ca/search/general/\" + str(page) + \"?q0.0=su%3A%22Ottawa+(Ont.)+--+Clubs.%22\"\n",
    "\n",
    "    pghtml = response.text\n",
    "\n",
    "    soup = BeautifulSoup(pghtml, 'lxml')\n",
    "\n",
    "    all_cards = soup.find_all(\"div\", attrs={\"class\": \"card\"})\n",
    "    \n",
    "    # because of how the page html is formatted, we first make a list of dictionaries, where each dictionary is one search result\n",
    "    for i in all_cards:\n",
    "        col = {}\n",
    "        dt = i.find_all(\"dt\") \n",
    "        dd = i.find_all(\"dd\")\n",
    "        for j in range(len(dt)):\n",
    "            dd_mod = dd[j].text\n",
    "            # strips extra whitespace before and after text\n",
    "            dd_mod = dd_mod.strip()\n",
    "            # replaces any \"newline\" character that mess with formatting\n",
    "            dd_mod = dd_mod.replace(\"\\n\", \" \")\n",
    "            # replaces any tabs that mess with formatting\n",
    "            dd_mod = dd_mod.replace(\"\\t\", \"\")\n",
    "            col[dt[j].text] = dd_mod\n",
    "        data.append(col) \n",
    "    \n",
    "    # this just lets you know what page it's on (handy for big searches)\n",
    "    print('\\n' + str(page) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert your results into a data frame...\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...that you can now export as a .csv! we love pandas\n",
    "df.to_csv(r'example.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-xerox",
   "metadata": {},
   "source": [
    "A key part of this data set is the column containing URLs to all of the items. If you wanted to [download them](https://github.com/ChantalMB/HIST4916-Workbook/blob/master/scrapers/canadiana-item-scraper.ipynb), you now already have the list to do so in an automated way!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
